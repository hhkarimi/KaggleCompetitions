{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Inception for indentify fish species\n",
    "The Nature Conservency put out a competition on Kaggle to classify fish species according to images of captured fish on fishing boats [https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring].  Convolution Neural Nets are quite formdible for image classification problems.  In this approach, we take a pre-trained ConvNet (Google's Inception https://arxiv.org/pdf/1512.00567v3.pdf) and fine-tune the top layer for the given training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a validation set\n",
    "But first, we have to split the training data so there is a validation set.  Although it's best to use multiple validation/training sets (cross-validation), we will not do so here due to computation restrictions (I could really use a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "root_dir = '../data/train'\n",
    "train_dir = '../data/train_split'\n",
    "val_dir = '../data/val_split'\n",
    "\n",
    "FishNames = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n",
    "\n",
    "# initiate number of training and validation samples (used in keras)\n",
    "nbr_train_samples = 0\n",
    "nbr_val_samples = 0\n",
    "\n",
    "# choose training proportion\n",
    "split_proportion = 0.8\n",
    "\n",
    "for fish in FishNames:\n",
    "    if fish not in os.listdir(train_dir):\n",
    "        os.mkdir(os.path.join(train_dir, fish))\n",
    "        \n",
    "    # list of image files\n",
    "    total_images = os.listdir(os.path.join(root_dir, fish))\n",
    "    \n",
    "    # number of split proportion\n",
    "    nbr_train = int(len(total_images) * split_proportion)\n",
    "    \n",
    "    # shuffle list of iamges\n",
    "    np.random.shuffle(total_images)\n",
    "    \n",
    "    # split into train and validation sets\n",
    "    train_images = total_images[:nbr_train]\n",
    "    val_images = total_images[nbr_train:]\n",
    "    \n",
    "    for img in train_images: # loop over all the images\n",
    "        source = os.path.join(root_dir, fish, img)\n",
    "        target = os.path.join(train_dir, fish, img)\n",
    "        shutil.copy(source, target)\n",
    "        nbr_train_samples = nbr_train_samples + 1\n",
    "        \n",
    "    if fish not in os.listdir(val_dir):\n",
    "        os.mkdir(os.path.join(val_dir, fish))\n",
    "        \n",
    "    for img in val_images:\n",
    "        source = os.path.join(root_dir, fish, img)\n",
    "        target = os.path.join(val_dir, fish, img)\n",
    "        shutil.copy(source, target)\n",
    "        nbr_val_samples = nbr_val_samples + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Inception model as the base\n",
    "Fortunately, Inception is built into keras and can easily be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.layers import AveragePooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l1l2\n",
    "\n",
    "# input_shape explained in https://keras.io/layers/convolutional/\n",
    "img_width = 299 # cols\n",
    "img_height = 299 # rows\n",
    "nbr_epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "InceptionV3_base = InceptionV3(include_top=False, \n",
    "                                weights='imagenet',\n",
    "                                input_tensor=None,\n",
    "                                input_shape=(img_height, img_width, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add top block of neural nets and compile\n",
    "Pool together the convolution blocks from Inception.  To prevent overfitting, include dropout on the top layers.  Using $L1$ and $L2$ normalization is also possible, though I didn't find any perfomance benefits from doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = InceptionV3_base.get_layer(index=-1).output\n",
    "output = GlobalAveragePooling2D(name='avg_pool')(output)\n",
    "output = Dense(512, activation='relu', W_regularizer=l1l2(1e-4,1e-4),\n",
    "               name='topFC1')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(64, activation='relu', W_regularizer=l1l2(1e-4,1e-4),\n",
    "               name='topFC2')(output)\n",
    "output = Dropout(0.5)(output)\n",
    "output = Dense(8, activation='softmax', name='predictions')(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze the base lower\n",
    "Since weights are randomly initialized on the newly-added top layer, we first fine-tune these before updating the Inception base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in InceptionV3_base.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "InceptionV3_model = Model(input=InceptionV3_base.input, output=output)\n",
    "# InceptionV3_model.summary()\n",
    "    \n",
    "InceptionV3_model.compile(loss = 'categorical_crossentropy',\n",
    "                         optimizer = 'rmsprop',\n",
    "                         metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize and augment input data to keras\n",
    "Rather than loading the entire training set in memory, we can call images in batches from the disk using the `flow_from_directory` method.  The training data can also easily be augmented with a variety of different methods.  Note that the color scheme in the images must be rescaled to be compatible with Inception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3019 images belonging to 8 classes.\n",
      "Found 758 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "## set data generators\n",
    "# augmentation configuration for training:\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.1,\n",
    "                                   zoom_range=0.1,\n",
    "                                   rotation_range=10.0,\n",
    "                                   width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "# validation set will only be rescaled\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "## set up input from directory to convnet\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    train_dir,\n",
    "                    target_size = (img_width, img_height),\n",
    "                    batch_size = batch_size,\n",
    "                    shuffle = True,\n",
    "                    # save_to_dir = '../data/visualization',\n",
    "                    # save_prefix = 'aug',\n",
    "                    classes = FishNames,\n",
    "                    class_mode = 'categorical')\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "                    val_dir,\n",
    "                    target_size = (img_width, img_height),\n",
    "                    shuffle = True,\n",
    "                    # save_to_dir = '../data/visualization',\n",
    "                    # save_prefix = 'aug',\n",
    "                    classes = FishNames,\n",
    "                    class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the top layer\n",
    "We will use Stochastic Gradient Descent to for this training step so that we can avoid getting stuck in a local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3019/3019 [==============================] - 12840s - loss: 4.1825 - acc: 0.4846 - val_loss: 3.4877 - val_acc: 0.6425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11f786b00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InceptionV3_model.fit_generator(train_generator,\n",
    "                               samples_per_epoch = nbr_train_samples,\n",
    "                               nb_epoch = nbr_epochs,\n",
    "                               validation_data = val_generator,\n",
    "                               nb_val_samples = nbr_val_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze top blocks in the base\n",
    "Now that the top layer is trained, we go back to the Inception base and fine-tune the top couple of convolution blocks there.  In order to know where the blocks end, look for the `keras.enging.topology.Merge` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <keras.engine.topology.InputLayer object at 0x1081eccc0>\n",
      "1 <keras.layers.convolutional.Convolution2D object at 0x11693b240>\n",
      "2 <keras.layers.normalization.BatchNormalization object at 0x116c52978>\n",
      "3 <keras.layers.convolutional.Convolution2D object at 0x116cd1ac8>\n",
      "4 <keras.layers.normalization.BatchNormalization object at 0x116d6feb8>\n",
      "5 <keras.layers.convolutional.Convolution2D object at 0x116d8bef0>\n",
      "6 <keras.layers.normalization.BatchNormalization object at 0x116f25898>\n",
      "7 <keras.layers.pooling.MaxPooling2D object at 0x116f7ada0>\n",
      "8 <keras.layers.convolutional.Convolution2D object at 0x116fbef98>\n",
      "9 <keras.layers.normalization.BatchNormalization object at 0x116fe7eb8>\n",
      "10 <keras.layers.convolutional.Convolution2D object at 0x11701de80>\n",
      "11 <keras.layers.normalization.BatchNormalization object at 0x116cbdcf8>\n",
      "12 <keras.layers.pooling.MaxPooling2D object at 0x1170f0e80>\n",
      "13 <keras.layers.convolutional.Convolution2D object at 0x117306e80>\n",
      "14 <keras.layers.normalization.BatchNormalization object at 0x11736c828>\n",
      "15 <keras.layers.convolutional.Convolution2D object at 0x117192e80>\n",
      "16 <keras.layers.convolutional.Convolution2D object at 0x1173c1e80>\n",
      "17 <keras.layers.normalization.BatchNormalization object at 0x1171f7828>\n",
      "18 <keras.layers.normalization.BatchNormalization object at 0x117526828>\n",
      "19 <keras.layers.pooling.AveragePooling2D object at 0x11774ee48>\n",
      "20 <keras.layers.convolutional.Convolution2D object at 0x117134fd0>\n",
      "21 <keras.layers.convolutional.Convolution2D object at 0x11724be80>\n",
      "22 <keras.layers.convolutional.Convolution2D object at 0x117577e80>\n",
      "23 <keras.layers.convolutional.Convolution2D object at 0x117722dd8>\n",
      "24 <keras.layers.normalization.BatchNormalization object at 0x11715deb8>\n",
      "25 <keras.layers.normalization.BatchNormalization object at 0x1172b1828>\n",
      "26 <keras.layers.normalization.BatchNormalization object at 0x1175e2828>\n",
      "27 <keras.layers.normalization.BatchNormalization object at 0x1177c6828>\n",
      "28 <keras.engine.topology.Merge object at 0x1177f0dd8>\n",
      "29 <keras.layers.convolutional.Convolution2D object at 0x117c0a5c0>\n",
      "30 <keras.layers.normalization.BatchNormalization object at 0x117c3edd8>\n",
      "31 <keras.layers.convolutional.Convolution2D object at 0x1179cceb8>\n",
      "32 <keras.layers.convolutional.Convolution2D object at 0x117cf8ef0>\n",
      "33 <keras.layers.normalization.BatchNormalization object at 0x117a0ff28>\n",
      "34 <keras.layers.normalization.BatchNormalization object at 0x117d665c0>\n",
      "35 <keras.layers.pooling.AveragePooling2D object at 0x117f84da0>\n",
      "36 <keras.layers.convolutional.Convolution2D object at 0x117955c88>\n",
      "37 <keras.layers.convolutional.Convolution2D object at 0x117a86e48>\n",
      "38 <keras.layers.convolutional.Convolution2D object at 0x117d8e240>\n",
      "39 <keras.layers.convolutional.Convolution2D object at 0x117f6bef0>\n",
      "40 <keras.layers.normalization.BatchNormalization object at 0x11796be10>\n",
      "41 <keras.layers.normalization.BatchNormalization object at 0x117aeb828>\n",
      "42 <keras.layers.normalization.BatchNormalization object at 0x117f1f550>\n",
      "43 <keras.layers.normalization.BatchNormalization object at 0x117fd8908>\n",
      "44 <keras.engine.topology.Merge object at 0x11823eef0>\n",
      "45 <keras.layers.convolutional.Convolution2D object at 0x118461f60>\n",
      "46 <keras.layers.normalization.BatchNormalization object at 0x1184cd898>\n",
      "47 <keras.layers.convolutional.Convolution2D object at 0x1182b4a20>\n",
      "48 <keras.layers.convolutional.Convolution2D object at 0x1184f65c0>\n",
      "49 <keras.layers.normalization.BatchNormalization object at 0x118347ef0>\n",
      "50 <keras.layers.normalization.BatchNormalization object at 0x118688668>\n",
      "51 <keras.layers.pooling.AveragePooling2D object at 0x11878d898>\n",
      "52 <keras.layers.convolutional.Convolution2D object at 0x11820dcc0>\n",
      "53 <keras.layers.convolutional.Convolution2D object at 0x1183a7dd8>\n",
      "54 <keras.layers.convolutional.Convolution2D object at 0x1186b0dd8>\n",
      "55 <keras.layers.convolutional.Convolution2D object at 0x1187cdcc0>\n",
      "56 <keras.layers.normalization.BatchNormalization object at 0x11829fb00>\n",
      "57 <keras.layers.normalization.BatchNormalization object at 0x1184134a8>\n",
      "58 <keras.layers.normalization.BatchNormalization object at 0x1187415f8>\n",
      "59 <keras.layers.normalization.BatchNormalization object at 0x1187f8be0>\n",
      "60 <keras.engine.topology.Merge object at 0x1188216d8>\n",
      "61 <keras.layers.convolutional.Convolution2D object at 0x11890b588>\n",
      "62 <keras.layers.normalization.BatchNormalization object at 0x11896e048>\n",
      "63 <keras.layers.convolutional.Convolution2D object at 0x1189c6518>\n",
      "64 <keras.layers.normalization.BatchNormalization object at 0x118a27198>\n",
      "65 <keras.layers.convolutional.Convolution2D object at 0x118815630>\n",
      "66 <keras.layers.convolutional.Convolution2D object at 0x118a8cf28>\n",
      "67 <keras.layers.normalization.BatchNormalization object at 0x1188bb080>\n",
      "68 <keras.layers.normalization.BatchNormalization object at 0x118a73c88>\n",
      "69 <keras.layers.pooling.MaxPooling2D object at 0x118b47eb8>\n",
      "70 <keras.engine.topology.Merge object at 0x118b16c88>\n",
      "71 <keras.layers.convolutional.Convolution2D object at 0x118f22be0>\n",
      "72 <keras.layers.normalization.BatchNormalization object at 0x118f847b8>\n",
      "73 <keras.layers.convolutional.Convolution2D object at 0x118fa38d0>\n",
      "74 <keras.layers.normalization.BatchNormalization object at 0x118fddc88>\n",
      "75 <keras.layers.convolutional.Convolution2D object at 0x118bf3f28>\n",
      "76 <keras.layers.convolutional.Convolution2D object at 0x119192ef0>\n",
      "77 <keras.layers.normalization.BatchNormalization object at 0x118c36f98>\n",
      "78 <keras.layers.normalization.BatchNormalization object at 0x1191fe518>\n",
      "79 <keras.layers.convolutional.Convolution2D object at 0x118caceb8>\n",
      "80 <keras.layers.convolutional.Convolution2D object at 0x119329240>\n",
      "81 <keras.layers.normalization.BatchNormalization object at 0x118ceff28>\n",
      "82 <keras.layers.normalization.BatchNormalization object at 0x1193bb4a8>\n",
      "83 <keras.layers.pooling.AveragePooling2D object at 0x119593b00>\n",
      "84 <keras.layers.convolutional.Convolution2D object at 0x118b85ba8>\n",
      "85 <keras.layers.convolutional.Convolution2D object at 0x118d64e48>\n",
      "86 <keras.layers.convolutional.Convolution2D object at 0x119521da0>\n",
      "87 <keras.layers.convolutional.Convolution2D object at 0x1195bfdd8>\n",
      "88 <keras.layers.normalization.BatchNormalization object at 0x118b92208>\n",
      "89 <keras.layers.normalization.BatchNormalization object at 0x118dc9828>\n",
      "90 <keras.layers.normalization.BatchNormalization object at 0x119574438>\n",
      "91 <keras.layers.normalization.BatchNormalization object at 0x11962cf28>\n",
      "92 <keras.engine.topology.Merge object at 0x1196c0198>\n",
      "93 <keras.layers.convolutional.Convolution2D object at 0x119a4b240>\n",
      "94 <keras.layers.normalization.BatchNormalization object at 0x119adc898>\n",
      "95 <keras.layers.convolutional.Convolution2D object at 0x119b29748>\n",
      "96 <keras.layers.normalization.BatchNormalization object at 0x119b98710>\n",
      "97 <keras.layers.convolutional.Convolution2D object at 0x11981c898>\n",
      "98 <keras.layers.convolutional.Convolution2D object at 0x119be66d8>\n",
      "99 <keras.layers.normalization.BatchNormalization object at 0x1198b09e8>\n",
      "100 <keras.layers.normalization.BatchNormalization object at 0x119e536a0>\n",
      "101 <keras.layers.convolutional.Convolution2D object at 0x1198fca58>\n",
      "102 <keras.layers.convolutional.Convolution2D object at 0x119ea0668>\n",
      "103 <keras.layers.normalization.BatchNormalization object at 0x11996a860>\n",
      "104 <keras.layers.normalization.BatchNormalization object at 0x119f0b630>\n",
      "105 <keras.layers.pooling.AveragePooling2D object at 0x11a013710>\n",
      "106 <keras.layers.convolutional.Convolution2D object at 0x119693e80>\n",
      "107 <keras.layers.convolutional.Convolution2D object at 0x119991e10>\n",
      "108 <keras.layers.convolutional.Convolution2D object at 0x119f5ca90>\n",
      "109 <keras.layers.convolutional.Convolution2D object at 0x119fef8d0>\n",
      "110 <keras.layers.normalization.BatchNormalization object at 0x1196f5a58>\n",
      "111 <keras.layers.normalization.BatchNormalization object at 0x119a22908>\n",
      "112 <keras.layers.normalization.BatchNormalization object at 0x119fc5748>\n",
      "113 <keras.layers.normalization.BatchNormalization object at 0x11a200a20>\n",
      "114 <keras.engine.topology.Merge object at 0x11a24a908>\n",
      "115 <keras.layers.convolutional.Convolution2D object at 0x11a5f6f28>\n",
      "116 <keras.layers.normalization.BatchNormalization object at 0x11a63af98>\n",
      "117 <keras.layers.convolutional.Convolution2D object at 0x11a6afe48>\n",
      "118 <keras.layers.normalization.BatchNormalization object at 0x11a7147b8>\n",
      "119 <keras.layers.convolutional.Convolution2D object at 0x11a306ef0>\n",
      "120 <keras.layers.convolutional.Convolution2D object at 0x11a76eb70>\n",
      "121 <keras.layers.normalization.BatchNormalization object at 0x11a3734a8>\n",
      "122 <keras.layers.normalization.BatchNormalization object at 0x11a90efd0>\n",
      "123 <keras.layers.convolutional.Convolution2D object at 0x11a3d7da0>\n",
      "124 <keras.layers.convolutional.Convolution2D object at 0x11a967908>\n",
      "125 <keras.layers.normalization.BatchNormalization object at 0x11a42b3c8>\n",
      "126 <keras.layers.normalization.BatchNormalization object at 0x11a9d5940>\n",
      "127 <keras.layers.pooling.AveragePooling2D object at 0x11aaa2a20>\n",
      "128 <keras.layers.convolutional.Convolution2D object at 0x11a28df98>\n",
      "129 <keras.layers.convolutional.Convolution2D object at 0x11a479d68>\n",
      "130 <keras.layers.convolutional.Convolution2D object at 0x11aa20e80>\n",
      "131 <keras.layers.convolutional.Convolution2D object at 0x11ad1fdd8>\n",
      "132 <keras.layers.normalization.BatchNormalization object at 0x11a2b76d8>\n",
      "133 <keras.layers.normalization.BatchNormalization object at 0x11a4bdba8>\n",
      "134 <keras.layers.normalization.BatchNormalization object at 0x11aa8cb00>\n",
      "135 <keras.layers.normalization.BatchNormalization object at 0x11ad29e80>\n",
      "136 <keras.engine.topology.Merge object at 0x11ad92eb8>\n",
      "137 <keras.layers.convolutional.Convolution2D object at 0x11b17cd68>\n",
      "138 <keras.layers.normalization.BatchNormalization object at 0x11b1c1ba8>\n",
      "139 <keras.layers.convolutional.Convolution2D object at 0x11b338f28>\n",
      "140 <keras.layers.normalization.BatchNormalization object at 0x11b37cf98>\n",
      "141 <keras.layers.convolutional.Convolution2D object at 0x11ae4df60>\n",
      "142 <keras.layers.convolutional.Convolution2D object at 0x11b3f1e48>\n",
      "143 <keras.layers.normalization.BatchNormalization object at 0x11ae2b208>\n",
      "144 <keras.layers.normalization.BatchNormalization object at 0x11b4577b8>\n",
      "145 <keras.layers.convolutional.Convolution2D object at 0x11b057668>\n",
      "146 <keras.layers.convolutional.Convolution2D object at 0x11b4b0b70>\n",
      "147 <keras.layers.normalization.BatchNormalization object at 0x11aee5390>\n",
      "148 <keras.layers.normalization.BatchNormalization object at 0x11b511fd0>\n",
      "149 <keras.layers.pooling.AveragePooling2D object at 0x11b724e80>\n",
      "150 <keras.layers.convolutional.Convolution2D object at 0x11add7128>\n",
      "151 <keras.layers.convolutional.Convolution2D object at 0x11b0c2ef0>\n",
      "152 <keras.layers.convolutional.Convolution2D object at 0x11b569908>\n",
      "153 <keras.layers.convolutional.Convolution2D object at 0x11b765eb8>\n",
      "154 <keras.layers.normalization.BatchNormalization object at 0x11ae03e48>\n",
      "155 <keras.layers.normalization.BatchNormalization object at 0x11b1304a8>\n",
      "156 <keras.layers.normalization.BatchNormalization object at 0x11b5d7940>\n",
      "157 <keras.layers.normalization.BatchNormalization object at 0x11b790a58>\n",
      "158 <keras.engine.topology.Merge object at 0x11b7dcf60>\n",
      "159 <keras.layers.convolutional.Convolution2D object at 0x11ba50fd0>\n",
      "160 <keras.layers.normalization.BatchNormalization object at 0x11ba2e1d0>\n",
      "161 <keras.layers.convolutional.Convolution2D object at 0x11baea358>\n",
      "162 <keras.layers.normalization.BatchNormalization object at 0x11bac8828>\n",
      "163 <keras.layers.convolutional.Convolution2D object at 0x11b91ffd0>\n",
      "164 <keras.layers.convolutional.Convolution2D object at 0x11bbcaeb8>\n",
      "165 <keras.layers.normalization.BatchNormalization object at 0x11b94bf60>\n",
      "166 <keras.layers.normalization.BatchNormalization object at 0x11bc37470>\n",
      "167 <keras.layers.convolutional.Convolution2D object at 0x11b996e80>\n",
      "168 <keras.layers.convolutional.Convolution2D object at 0x11bc54b38>\n",
      "169 <keras.layers.normalization.BatchNormalization object at 0x11b96b9e8>\n",
      "170 <keras.layers.normalization.BatchNormalization object at 0x11bcf2390>\n",
      "171 <keras.layers.pooling.AveragePooling2D object at 0x11bd3def0>\n",
      "172 <keras.engine.topology.Merge object at 0x11bd82b00>\n",
      "173 <keras.layers.convolutional.Convolution2D object at 0x11c236a58>\n",
      "174 <keras.layers.normalization.BatchNormalization object at 0x11c2a4e80>\n",
      "175 <keras.layers.convolutional.Convolution2D object at 0x11bddf5c0>\n",
      "176 <keras.layers.convolutional.Convolution2D object at 0x11c2f0e48>\n",
      "177 <keras.layers.normalization.BatchNormalization object at 0x11be728d0>\n",
      "178 <keras.layers.normalization.BatchNormalization object at 0x11c45eac8>\n",
      "179 <keras.layers.convolutional.Convolution2D object at 0x11be99d68>\n",
      "180 <keras.layers.convolutional.Convolution2D object at 0x11c17b6a0>\n",
      "181 <keras.layers.convolutional.Convolution2D object at 0x11c4729e8>\n",
      "182 <keras.layers.convolutional.Convolution2D object at 0x11c643828>\n",
      "183 <keras.layers.pooling.AveragePooling2D object at 0x11c6f5400>\n",
      "184 <keras.layers.convolutional.Convolution2D object at 0x11bd97390>\n",
      "185 <keras.layers.normalization.BatchNormalization object at 0x11c1296d8>\n",
      "186 <keras.layers.normalization.BatchNormalization object at 0x11c1e85f8>\n",
      "187 <keras.layers.normalization.BatchNormalization object at 0x11c6199e8>\n",
      "188 <keras.layers.normalization.BatchNormalization object at 0x11c639a58>\n",
      "189 <keras.layers.convolutional.Convolution2D object at 0x11c771860>\n",
      "190 <keras.layers.normalization.BatchNormalization object at 0x11bdb7898>\n",
      "191 <keras.engine.topology.Merge object at 0x11c236eb8>\n",
      "192 <keras.engine.topology.Merge object at 0x11c766668>\n",
      "193 <keras.layers.normalization.BatchNormalization object at 0x11c791a58>\n",
      "194 <keras.engine.topology.Merge object at 0x11c7dc940>\n",
      "195 <keras.layers.convolutional.Convolution2D object at 0x11cc0df60>\n",
      "196 <keras.layers.normalization.BatchNormalization object at 0x11cc38e48>\n",
      "197 <keras.layers.convolutional.Convolution2D object at 0x11c99aeb8>\n",
      "198 <keras.layers.convolutional.Convolution2D object at 0x11cc84f60>\n",
      "199 <keras.layers.normalization.BatchNormalization object at 0x11ca07470>\n",
      "200 <keras.layers.normalization.BatchNormalization object at 0x11cc60208>\n",
      "201 <keras.layers.convolutional.Convolution2D object at 0x11ca26b38>\n",
      "202 <keras.layers.convolutional.Convolution2D object at 0x11cb0eef0>\n",
      "203 <keras.layers.convolutional.Convolution2D object at 0x11cd8e668>\n",
      "204 <keras.layers.convolutional.Convolution2D object at 0x11cdf9ef0>\n",
      "205 <keras.layers.pooling.AveragePooling2D object at 0x11cfd0a90>\n",
      "206 <keras.layers.convolutional.Convolution2D object at 0x11c921fd0>\n",
      "207 <keras.layers.normalization.BatchNormalization object at 0x11cac3390>\n",
      "208 <keras.layers.normalization.BatchNormalization object at 0x11cb52fd0>\n",
      "209 <keras.layers.normalization.BatchNormalization object at 0x11cd1e390>\n",
      "210 <keras.layers.normalization.BatchNormalization object at 0x11cf6b4a8>\n",
      "211 <keras.layers.convolutional.Convolution2D object at 0x11d010f28>\n",
      "212 <keras.layers.normalization.BatchNormalization object at 0x11c94a6a0>\n",
      "213 <keras.engine.topology.Merge object at 0x11cbc9ef0>\n",
      "214 <keras.engine.topology.Merge object at 0x11cfd0da0>\n",
      "215 <keras.layers.normalization.BatchNormalization object at 0x11d031b00>\n",
      "216 <keras.engine.topology.Merge object at 0x11d044a20>\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(InceptionV3_base.layers):\n",
    "    print(i, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the top two convolution blocks free for further training, we unfreeze layers 172 and onwards (since 172 is the last Merge object in the fixed convolution block)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in InceptionV3_model.layers[:172]:\n",
    "    layer.trainable = False\n",
    "for layer in InceptionV3_model.layers[172:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# recompile so the modifications take effect\n",
    "optimizer = SGD(lr=1e-4, momentum=0.9, decay=0.0, nesterov=True)\n",
    "InceptionV3_model.compile(optimizer = optimizer,\n",
    "                         loss = 'categorical_crossentropy',\n",
    "                         metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "Fit the model and save the best performer in terms of validation accuracy over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2987/3019 [============================>.] - ETA: 18s - loss: 3.8079 - acc: 0.5504Epoch 00000: val_acc improved from -inf to 0.66227, saving model to best_weights.h5\n",
      "3019/3019 [==============================] - 2115s - loss: 3.8073 - acc: 0.5502 - val_loss: 3.4817 - val_acc: 0.6623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12c717f98>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_file = 'best_weights.h5'\n",
    "best_model = ModelCheckpoint(best_model_file, monitor='val_acc', \n",
    "                             verbose=1, save_best_only=True)\n",
    "\n",
    "InceptionV3_model.fit_generator(train_generator,\n",
    "                               samples_per_epoch = nbr_train_samples,\n",
    "                               nb_epoch = nbr_epochs,\n",
    "                               validation_data = val_generator,\n",
    "                               nb_val_samples = nbr_val_samples,\n",
    "                               callbacks = [best_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "Although the validation was scored without any image alteration, we will apply augmentation averaging over the test data.  The idea is to alter the training data by augmentation methods before predicting their class according to the model.  By iterating over different augmentation combinations, our averaged prediction outperforms a straight-foward prediction scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build the test generator\n",
    "nbr_test_samples = 1000\n",
    "test_data_dir = '../data/test_stg1/'\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                 shear_range=0.1,\n",
    "                                 zoom_range=0.1,\n",
    "                                 width_shift_range=0.1,\n",
    "                                 height_shift_range=0.1,\n",
    "                                 horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st augmentation for testing ...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "2th augmentation for testing ...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "3th augmentation for testing ...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "4th augmentation for testing ...\n",
      "Found 1000 images belonging to 1 classes.\n",
      "5th augmentation for testing ...\n",
      "Found 1000 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "nbr_augmentation = 5\n",
    "for idx in range(nbr_augmentation):\n",
    "    if idx == 0: \n",
    "        print('{}st augmentation for testing ...'.format(idx+1))\n",
    "    else:\n",
    "        print('{}th augmentation for testing ...'.format(idx+1))\n",
    "    random_seed = 21\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "                    test_data_dir,\n",
    "                    target_size = (img_width, img_height),\n",
    "                    batch_size = batch_size,\n",
    "                    shuffle = False,\n",
    "                    seed = random_seed,\n",
    "                    classes = None,\n",
    "                    class_mode = None)\n",
    "    \n",
    "    test_image_list = test_generator.filenames\n",
    "\n",
    "    if idx == 0:\n",
    "        predictions = InceptionV3_model.predict_generator(test_generator, \n",
    "                                                          nbr_test_samples)\n",
    "    else:\n",
    "        predictions += InceptionV3_model.predict_generator(test_generator,\n",
    "                                                          nbr_test_samples)    \n",
    "        \n",
    "predictions /= nbr_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the submission file\n",
    "Following the Kaggle submission guidelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_submit = open('submit_InceptionBasedFishing.csv', 'w')\n",
    "f_submit.write('image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\\n')\n",
    "for i, image_name in enumerate(test_image_list):\n",
    "    pred = ['%.6f' % p for p in predictions[i,:]]\n",
    "    f_submit.write('%s,%s\\n' % (os.path.basename(image_name),\n",
    "                                ','.join(pred)))\n",
    "    \n",
    "f_submit.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "After training for 40 epochs, the validation loss decreased to ~0.20 and the accuracy improved to ~97%.  When testing on Kaggle's test data, the model scored 0.99 on categorical cross-entropy loss (Top 9% as of this writing).  This problem of overfitting the training data is most likely due to the fact that the photos in the test set are taken on different boats.  The best way to avoid this issue to by first implementing a fish detection system then sending the cropped fish through the neural net for training and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* Fine-tuning Inception on keras: <br>\n",
    "    https://keras.io/applications/#fine-tune-inceptionv3-on-a-new-set-of-classes\n",
    "        \n",
    "* Pre-processing data: <br>\n",
    "    https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring/forums/t/26202\n",
    "    \n",
    "* Google's Inception model: <br>\n",
    "    https://arxiv.org/pdf/1512.00567v3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
